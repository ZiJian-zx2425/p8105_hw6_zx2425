---
title: "Simple document"
output: github_document

    
---
```{r,echo=FALSE, message=FALSE}
library(tidyverse)
library(patchwork)
library(p8105.datasets)
library(tidyverse)
library(modelr)
library(mgcv)
library(purrr)
```

let's begin

# Problem 1

To obtain a distribution for $\hat{r}^2$, we'll follow basically the same procedure we used for regression coefficients: draw bootstrap samples; the a model to each; extract the value I'm concerned with; and summarize. Here, we'll use `modelr::bootstrap` to draw the samples and `broom::glance` to produce `r.squared` values. 

```{r weather_df, cache = TRUE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```


```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

In this example, the $\hat{r}^2$ value is high, and the upper bound at 1 may be a cause for the generally skewed shape of the distribution. If we wanted to construct a confidence interval for $R^2$, we could take the 2.5% and 97.5% quantiles of the estimates across bootstrap samples. However, because the shape isn't symmetric, using the mean +/- 1.96 times the standard error probably wouldn't work well.

We can produce a distribution for $\log(\beta_0 * \beta1)$ using a similar approach, with a bit more wrangling before we make our plot.

```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```

As with $r^2$, this distribution is somewhat skewed and has some outliers. 

The point of this is not to say you should always use the bootstrap -- it's possible to establish "large sample" distributions for strange parameters / values / summaries in a lot of cases, and those are great to have. But it is helpful to know that there's a way to do inference even in tough cases. 





```{r}
set.seed(001)
```

# problem2
## 2.1 tidy data
REQUIREMENT:Create a _city_state_ variable (e.g. “Baltimore, MD”), and _a binary variable_ indicating whether the homicide is solved. 
```{r}
homicide = read.csv("./homicide-data.csv") 

summary(homicide$disposition)
homicide=homicide%>% 
  na.omit() %>% 
  mutate(
    city_state=str_c(city,", ",state)
  ) %>% 
  mutate(solve_sit = case_when(
    disposition== "Closed by arrest" ~ 1,
    disposition== "Open/No arrest" ~ 0,
     disposition== "Closed without arrest" ~ 0)) 
```
Based on the requirement of problem2.1, we give a numerical definition for disposition. However, with recognizing that most of the disposition data including three kinds of description:"Closed by arrest", "Open/No arrest" and "Closed without arrest", We consider the first as resolved event and the others as unresolved event.

REQUIREMENT:Omit cities Dallas, TX; Phoenix, AZ; and Kansas City, MO – these don’t report victim race. Also omit Tulsa, AL – this is a data entry mistake. For this problem, limit your analysis those for whom victim_race is white or black. Be sure that victim_age is numeric.

```{r}
homicide =homicide%>% 
filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL")) %>% 
  filter(victim_race %in% c("White", "Black")) %>% 
  filter(!victim_sex=="Unknown") %>% 
  mutate(
    victim_age = as.numeric(victim_age),
    victim_sex = as.factor(victim_sex),
    victim_race = as.factor(victim_race),
    )
homicide = homicide %>% 
  filter(city_state !="Tulsa, AL")
```
Based on the requirements for the cit_state and race, we finally obtain the results data: _homicide_ 

## 2.1 build linear regression model
REQUIREMENT:For the city of Baltimore, MD, use the glm function to fit a logistic regression with _resolved vs unresolved_ as the outcome and _victim_ _age_, _sex_ and _race_ as predictors. 
```{r}
data_B= filter(homicide,city_state=="Baltimore, MD")
data_B_model = data_B%>% 
  glm(solve_sit ~ victim_age + victim_race + victim_sex, data = ., family = binomial())
```
REQUIRMENT:Save the output of glm as an R object; 
```{r}
save(data_B_model, file = "./data_B_model.RData")
data_B_model
```
REQUIREMENT:apply the _broom::tidy_ to this object; and obtain the _estimate_ and _confidence interval_ of the adjusted odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed.
```{r}
data_B_model %>% 
  broom::tidy(conf.int = TRUE) %>% 
  mutate(
  OR = exp(estimate), 
  conf_low=exp(estimate+qnorm(0.05)*std.error), 
  conf_high=exp(estimate+qnorm(0.95)*std.error)
  ) %>%
  select(term, log_OR = estimate, OR, p.value, conf_low, conf_high) %>% 
  knitr::kable(digits = 3)

data_B_model %>% 
  broom::tidy(conf.int = TRUE) %>% 
  mutate(
  OR = exp(estimate), 
  conf_low=exp(estimate+qnorm(0.025)*std.error), 
  conf_high=exp(estimate+qnorm(0.975)*std.error)
  ) %>%
  filter(term=="victim_sexMale") %>% 
  select(term, log_OR = estimate, OR, p.value, conf_low, conf_high) %>% 
  knitr::kable(digits = 3)
```

REQUIREMENT:Now run glm for each of the cities in your dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims. 
Do this within a “tidy” pipeline, making use of purrr::map, list columns, and unnest as necessary to create a dataframe with estimated ORs and CIs for each city.


```{r}
all_city_model = 
   homicide %>%
  select(city_state,solve_sit,victim_age,victim_sex,victim_race) %>% 
  nest(data = -city_state) %>% 
  mutate(
    logmodel = map(data, ~glm(solve_sit~victim_age + victim_race + victim_sex, data = ., family = binomial())),
 result = map(.x=logmodel, ~broom::tidy(.x,conf.int = TRUE))) %>% 
  select(-data,-logmodel)%>% 
  unnest(result)%>% 
  mutate(
    OR = exp(estimate), 
    conf_low=exp(estimate+qnorm(0.05)*std.error), 
    conf_high=exp(estimate+qnorm(0.95)*std.error)
    ) %>%
   filter(term=="victim_sexMale") %>% 
  select(city_state, OR,conf_low, conf_high) 
```
REQUIREMENT:Create a plot that shows the estimated ORs and CIs for each city. Organize cities according to estimated OR, and comment on the plot
```{r}
all_city_model%>%
  mutate( 
    city_state = fct_reorder(city_state, OR)
  ) %>%
  ggplot(aes(x = city_state, y = OR )) +
  geom_point() +
  geom_errorbar(aes(ymin = conf_low, ymax = conf_high)) +
  labs(title = "OR of solved cases for male and female victims",
       x="Odds.ratio",
       y="City-states"
         )+
  theme(
    title=element_text(size=9),
    axis.text.x.bottom = element_text(angle=45, hjust=1)
  )
```

Execute the function on the data of each city, and calculate the corresponding OR and its CI. Then we sort and draw the image according to the value of OR
#problem3
Load and clean the data for regression analysis (i.e. convert numeric to factor where appropriate, check for missing data, etc.).
## 3.0 reclaim data
```{r}
data_bw <- read.csv("./birthweight.csv")%>% 
  na.omit() %>% 
  mutate(
    babysex = as.factor(babysex),
    frace = as.factor(frace), 
    malform = as.factor(malform),
    mrace = factor(mrace)
    )
```

Propose a _regression model_ for birthweight. This model may be based on a hypothesized structure for the factors that underly birthweight, on a data-driven model-building process, or a combination of the two. 
## 3.1 variables visualization

### 3.1.1 dependent variable visual
```{r}
density_bwt <- data_bw %>% 
  ggplot(aes(x = bwt)) + 
  geom_histogram()
density_bwt
```

From the plot, we can roughly assume that the distribution of bwt follows normal distribution.

### 3.1.2 Independent variable visual

```{r}
density_bhead <- data_bw %>% 
  ggplot(aes(x = bhead)) + 
  geom_histogram()
density_blength <- data_bw %>% 
  ggplot(aes(x = blength)) + 
  geom_histogram()
density_bhead+density_blength
```

From the density distribution plot, we can see that the distribution of bhead variable and blength variable is normal. Thus, we can directly put them into our linear regression model.

## 3.1.3 the relation ship among varaibles
```{r}
data_bw1=data_bw %>% 
  select(bwt,bhead,blength)
plot(data_bw1)
```

From the plot, we can be sure that there is relationship between bwt and blength and the same for bhead. So next step, we can use linear regression model to get the further development,

## 3.2 linear regression
```{r}
set.seed(002)
fit_headlength = lm(bwt ~ bhead+blength, data = data_bw)
summary(fit_headlength)
fit_headlength %>% 
  broom::glance()%>% 
  broom::tidy()
```

Describe your modeling process and show a plot of model _residuals against fitted values_ – _use add_predictions_ and _add_residuals_ in making this plot.
## 3.3 diagnosis for linear regression model 
```{r}
data_bw %>% 
  modelr::add_residuals(fit_headlength) %>% 
  modelr::add_predictions(fit_headlength) %>%
  ggplot(aes(x = pred, y = resid)) + 
  geom_point()+
  labs(
   title="head-lengh of residual vs fitted value",
   y="Residual" ,
   x="fitted value"
  )
  
```

First, we verified the assumption that the variables basically satisfy the normal distribution. Next, we drew a scatter diagram of the variables in pairs to observe whether the two independent variables have an impact on the dependent variable, and roughly estimate whether the impact is positive or negative.Further, we performed multiple linear regression and finally got the linear model.

From the residuals vs fitted value graph, we can see that, first of all, the data satisfies a symmetrical distribution around 0. But as the fitted value increases, the data becomes more dense. This shows that there are outliers in the fitted value.

## 3.4 manipulate three models and get the results
Compare your model to two others:
1.One using _length at birth_ and _gestational age_ as predictors (main effects only)
2.One using _head circumference_, _length_, _sex_, and _all interactions_ (including the three-way interaction) between these


```{r}
fit_lg = lm(bwt ~ gaweeks + blength, data =data_bw)
summary(fit_lg)
fit_lg %>% 
  broom::glance()%>% 
  broom::tidy()

fit_all = lm(bwt ~ bhead * blength * babysex, data = data_bw)
summary(fit_all)
fit_lg %>% 
  broom::glance()%>% 
  broom::tidy()
```

```{r}

cv_df =
  crossv_mc(data_bw, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))
cv_df = 
  cv_df %>% 
  mutate(
    model_bl = map(train, ~lm(bwt ~ bhead + blength, data = .x)),
    model_lg = map(train, ~lm(bwt ~ gaweeks + blength, data = .x)),
    model_all = map(train, ~lm(bwt ~ bhead * blength * babysex, data = .x))) %>% 
  mutate(
    rmse_bl = map2_dbl(model_bl, test, ~rmse(model = .x, data = .y)),
    rmse_lg = map2_dbl(model_lg, test, ~rmse(model = .x, data = .y)),
    rmse_all = map2_dbl(model_all, test, ~rmse(model = .x, data = .y)))
cv_df

cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

From the violin box plot, we can easily see that model_bl and model_all have smaller rmse than model_lg. And model_bl has the almost best performance in all ways. This might because our model_bl and model_lg have more " useful" varaibles for prediction.
